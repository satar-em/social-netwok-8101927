{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# SN_HW3\n",
    "\n",
    "```Networks``` folder should be near this notebook"
   ],
   "id": "be33295ddeed8f24"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imports & Setup",
   "id": "9ccfcedd05ae1fa2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple, List, Set, Optional\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.rcParams[\"figure.dpi\"] = 100\n",
    "\n",
    "# RANDOM_SEED = 42\n",
    "# random.seed(RANDOM_SEED)\n",
    "# np.random.seed(RANDOM_SEED)\n",
    "\n",
    "\n",
    "BASE_DIR = Path(\".\").resolve()\n",
    "\n",
    "OUTPUT_DIR = BASE_DIR / \"outputs\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "NETWORKS_DIR = BASE_DIR / \"Networks\"\n"
   ],
   "id": "96b459d2f37475ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Q2",
   "id": "4edd92815f290b2a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Locate",
   "id": "5a40f19617189a84"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "OUTPUT_DIR_Q2 = OUTPUT_DIR / 'Q2'\n",
    "OUTPUT_DIR_Q2.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PART_B_DIR = NETWORKS_DIR / 'Part_B'\n",
    "\n",
    "print('OUTPUT_DIR_Q2:', OUTPUT_DIR_Q2)\n",
    "print('NETWORKS_DIR:', NETWORKS_DIR)\n",
    "print('PART_B_DIR:', PART_B_DIR)\n",
    "print('Part_B files (sample):', sorted([p.name for p in PART_B_DIR.glob('*.csv')])[:10])\n"
   ],
   "id": "976456bf96050ed9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Helper Functions",
   "id": "1e50436ad16f110b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def _normalize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = [str(c).strip().lower() for c in df.columns]\n",
    "    return df\n",
    "\n",
    "def _detect_id_column(df: pd.DataFrame) -> str:\n",
    "    for cand in ['id', 'student_id', 'student', 'node', 'uid']:\n",
    "        if cand in df.columns:\n",
    "            return cand\n",
    "    raise ValueError(f'Could not find an ID column. Available columns: {df.columns.tolist()}')\n",
    "\n",
    "def _detect_edge_columns(df: pd.DataFrame) -> Tuple[str, str]:\n",
    "    cols = df.columns.tolist()\n",
    "    for a, b in [('student_1','student_2'), ('u','v'), ('i','j'), ('node_1','node_2'), ('node_i','node_j')]:\n",
    "        if a in cols and b in cols:\n",
    "            return a, b\n",
    "    if len(cols) >= 2:\n",
    "        return cols[0], cols[1]\n",
    "    raise ValueError(f'Connection file has <2 columns: {cols}')\n",
    "\n",
    "def load_snapshot(day_token: str) -> Tuple[nx.Graph, pd.DataFrame]:\n",
    "    conn_path = PART_B_DIR / f'connections_{day_token}.csv'\n",
    "    prop_path = PART_B_DIR / f'properties_{day_token}.csv'\n",
    "\n",
    "    if not conn_path.exists() or not prop_path.exists():\n",
    "        raise FileNotFoundError(f'Missing files for {day_token}: {conn_path.name}, {prop_path.name}')\n",
    "\n",
    "    df_props = _normalize_columns(pd.read_csv(prop_path))\n",
    "    id_col = _detect_id_column(df_props)\n",
    "    df_props = df_props.set_index(id_col).sort_index()\n",
    "\n",
    "    df_conn = _normalize_columns(pd.read_csv(conn_path))\n",
    "    u_col, v_col = _detect_edge_columns(df_conn)\n",
    "\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(df_props.index.tolist())\n",
    "\n",
    "    for node_id, row in df_props.iterrows():\n",
    "        G.nodes[node_id].update(row.to_dict())\n",
    "\n",
    "    edges = list(zip(df_conn[u_col].tolist(), df_conn[v_col].tolist()))\n",
    "    G.add_edges_from(edges)\n",
    "\n",
    "    for u, v in edges:\n",
    "        if u not in G:\n",
    "            G.add_node(u)\n",
    "        if v not in G:\n",
    "            G.add_node(v)\n",
    "\n",
    "    return G, df_props\n",
    "\n",
    "def discover_day_tokens() -> List[str]:\n",
    "    conn_files = list(PART_B_DIR.glob('connections_*.csv'))\n",
    "    tokens = []\n",
    "    for p in conn_files:\n",
    "        token = p.stem.replace('connections_', '')\n",
    "        # require matching properties file\n",
    "        if (PART_B_DIR / f'properties_{token}.csv').exists():\n",
    "            tokens.append(token)\n",
    "\n",
    "    def sort_key(tok: str):\n",
    "        nums = [int(x) for x in tok.replace('-', '_').split('_') if x.isdigit()]\n",
    "        return nums[0] if nums else 10**9\n",
    "\n",
    "    tokens = sorted(set(tokens), key=sort_key)\n",
    "    return tokens\n",
    "\n",
    "def safe_binary_series(s: pd.Series) -> pd.Series:\n",
    "    if s.dropna().isin([0,1,0.0,1.0,True,False]).all():\n",
    "        return s.astype(int)\n",
    "    return s\n",
    "\n",
    "def save_df(df: pd.DataFrame, path: Path) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(path, index=False)\n",
    "    print(f\"***file saved*** => {path}\")\n",
    "\n"
   ],
   "id": "324285e28bb39370",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load All Time Steps",
   "id": "37df0b8f5a2f4184"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "day_tokens = discover_day_tokens()\n",
    "if not day_tokens:\n",
    "    raise FileNotFoundError('No snapshots found in Part_B (expected connections_*.csv and properties_*.csv).')\n",
    "\n",
    "data: Dict[str, Dict[str, object]] = {}\n",
    "for tok in day_tokens:\n",
    "    G, df = load_snapshot(tok)\n",
    "    # normalize typical columns if present\n",
    "    for col in df.columns:\n",
    "        if col in ['smokes', 'club', 'plays_football', 'watches_movies']:\n",
    "            df[col] = safe_binary_series(df[col])\n",
    "    data[tok] = {'G': G, 'df': df}\n",
    "    print(f'Loaded {tok}: |V|={G.number_of_nodes()}, |E|={G.number_of_edges()}, props_cols={len(df.columns)}')\n",
    "\n",
    "print('Snapshots:', day_tokens)\n"
   ],
   "id": "79174631373b9232",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Part 1 — Feature Distribution & Evolution",
   "id": "e1fd2e5ed8db219d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ---- 1) Smoking evolution + study comparison ----\n",
    "stats_rows = []\n",
    "\n",
    "# Detect main columns (try common names)\n",
    "def pick_column(df: pd.DataFrame, candidates: List[str]) -> Optional[str]:\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "for tok in day_tokens:\n",
    "    df = data[tok]['df']\n",
    "    smokes_col = pick_column(df, ['smokes', 'smoker', 'smoking'])\n",
    "    studies_col = pick_column(df, ['studies', 'study', 'study_level', 'hours_study', 'study_hours'])\n",
    "\n",
    "    if smokes_col is None:\n",
    "        raise ValueError(f\"Column for smoking not found in properties_{tok}.csv\")\n",
    "\n",
    "    smokes = safe_binary_series(df[smokes_col])\n",
    "    num_smokers = int(smokes.sum())\n",
    "    pct_smokers = float(num_smokers / len(df) * 100.0)\n",
    "\n",
    "    avg_studies_smokers = np.nan\n",
    "    avg_studies_non_smokers = np.nan\n",
    "    if studies_col is not None:\n",
    "        avg_studies_smokers = float(df.loc[smokes == 1, studies_col].mean())\n",
    "        avg_studies_non_smokers = float(df.loc[smokes == 0, studies_col].mean())\n",
    "\n",
    "    stats_rows.append({\n",
    "        'snapshot': tok,\n",
    "        'n_students': int(len(df)),\n",
    "        'num_smokers': num_smokers,\n",
    "        'pct_smokers': pct_smokers,\n",
    "        'avg_studies_smokers': avg_studies_smokers,\n",
    "        'avg_studies_non_smokers': avg_studies_non_smokers,\n",
    "    })\n",
    "\n",
    "df_stats = pd.DataFrame(stats_rows)\n",
    "save_df(df_stats,OUTPUT_DIR_Q2 / 'smoker_evolution_stats.csv')\n",
    "\n",
    "# Plot smoker count\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(df_stats['snapshot'], df_stats['num_smokers'], marker='o')\n",
    "plt.title('Number of Smokers Over Time')\n",
    "plt.xlabel('Snapshot')\n",
    "plt.ylabel('Count')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR_Q2 / 'plot_smoker_count.png', dpi=200)\n",
    "print(f\"***file saved*** => {(OUTPUT_DIR_Q2 / 'plot_smoker_count.png')}\")\n",
    "# plt.show()\n",
    "plt.close()\n",
    "\n",
    "if not df_stats['avg_studies_smokers'].isna().all():\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(df_stats['snapshot'], df_stats['avg_studies_smokers'], marker='o', label='Smokers')\n",
    "    plt.plot(df_stats['snapshot'], df_stats['avg_studies_non_smokers'], marker='s', label='Non-Smokers')\n",
    "    plt.title('Average Study Level: Smokers vs Non-Smokers')\n",
    "    plt.xlabel('Snapshot')\n",
    "    plt.ylabel('Study (mean)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTPUT_DIR_Q2 / 'plot_study_habits.png', dpi=200)\n",
    "    print(f\"***file saved*** => {(OUTPUT_DIR_Q2 / 'plot_study_habits.png')}\")\n",
    "    # plt.show()\n",
    "    plt.close()\n",
    "else:\n",
    "    print('Study column not found; skipping study comparison plot.')\n"
   ],
   "id": "9b26ad164e6783db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Part 2 — Closure Mechanisms (Triadic, Membership, Focal)\n",
   "id": "3d2e07d97b460241"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def choose_membership_column(df: pd.DataFrame) -> Optional[str]:\n",
    "    # Prefer class-like membership\n",
    "    for c in ['class_number', 'class', 'classid', 'grade', 'room', 'section']:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def choose_interest_columns(df: pd.DataFrame) -> List[str]:\n",
    "    exclude = {'smokes', 'smoker', 'smoking', 'gender', 'sex', 'class_number', 'class', 'studies', 'study', 'study_level'}\n",
    "    cols = []\n",
    "    for c in df.columns:\n",
    "        if c in exclude:\n",
    "            continue\n",
    "        s = df[c]\n",
    "        if s.dropna().isin([0,1,0.0,1.0,True,False]).all():\n",
    "            cols.append(c)\n",
    "    for c in ['plays_football', 'watches_movies', 'club']:\n",
    "        if c in df.columns and c not in cols:\n",
    "            cols.append(c)\n",
    "    return cols\n",
    "\n",
    "def edge_is_triadic(G_prev: nx.Graph, u, v) -> bool:\n",
    "    if u not in G_prev or v not in G_prev:\n",
    "        return False\n",
    "    Nu = set(G_prev.neighbors(u))\n",
    "    Nv = set(G_prev.neighbors(v))\n",
    "    return len(Nu & Nv) > 0\n",
    "\n",
    "def edge_is_membership(df_prev: pd.DataFrame, u, v, membership_col: str) -> bool:\n",
    "    if membership_col is None:\n",
    "        return False\n",
    "    if u not in df_prev.index or v not in df_prev.index:\n",
    "        return False\n",
    "    return df_prev.loc[u, membership_col] == df_prev.loc[v, membership_col]\n",
    "\n",
    "def edge_is_focal(df_prev: pd.DataFrame, u, v, interest_cols: List[str]) -> bool:\n",
    "    if not interest_cols:\n",
    "        return False\n",
    "    if u not in df_prev.index or v not in df_prev.index:\n",
    "        return False\n",
    "    for c in interest_cols:\n",
    "        try:\n",
    "            if int(df_prev.loc[u, c]) == 1 and int(df_prev.loc[v, c]) == 1:\n",
    "                return True\n",
    "        except Exception:\n",
    "            continue\n",
    "    return False\n",
    "\n",
    "closure_rows = []\n",
    "\n",
    "for i in range(len(day_tokens) - 1):\n",
    "    t_prev = day_tokens[i]\n",
    "    t_curr = day_tokens[i + 1]\n",
    "\n",
    "    G_prev = data[t_prev]['G']\n",
    "    G_curr = data[t_curr]['G']\n",
    "    df_prev = data[t_prev]['df']\n",
    "\n",
    "    prev_edges = set(map(tuple, map(sorted, G_prev.edges())))\n",
    "    curr_edges = set(map(tuple, map(sorted, G_curr.edges())))\n",
    "\n",
    "    new_edges = [e for e in curr_edges if e not in prev_edges]\n",
    "\n",
    "    # Columns\n",
    "    smokes_col = None\n",
    "    for c in ['smokes', 'smoker', 'smoking']:\n",
    "        if c in df_prev.columns:\n",
    "            smokes_col = c\n",
    "            break\n",
    "    if smokes_col is None:\n",
    "        raise ValueError(f\"Smoking column not found at {t_prev}\")\n",
    "\n",
    "    membership_col = choose_membership_column(df_prev)\n",
    "    interest_cols = choose_interest_columns(df_prev)\n",
    "\n",
    "    triadic_count = 0\n",
    "    membership_count = 0\n",
    "    focal_count = 0\n",
    "\n",
    "    smoker_involved_count = 0\n",
    "    smoker_triadic = 0\n",
    "    smoker_focal = 0\n",
    "\n",
    "    for u, v in new_edges:\n",
    "        is_triadic = edge_is_triadic(G_prev, u, v)\n",
    "        is_membership = edge_is_membership(df_prev, u, v, membership_col) if membership_col else False\n",
    "        is_focal = edge_is_focal(df_prev, u, v, interest_cols)\n",
    "\n",
    "        if is_triadic:\n",
    "            triadic_count += 1\n",
    "        if is_membership:\n",
    "            membership_count += 1\n",
    "        if is_focal:\n",
    "            focal_count += 1\n",
    "\n",
    "        u_sm = int(df_prev.loc[u, smokes_col]) if u in df_prev.index else 0\n",
    "        v_sm = int(df_prev.loc[v, smokes_col]) if v in df_prev.index else 0\n",
    "        smoker_involved = (u_sm == 1) or (v_sm == 1)\n",
    "        if smoker_involved:\n",
    "            smoker_involved_count += 1\n",
    "            if is_triadic:\n",
    "                smoker_triadic += 1\n",
    "            if is_focal:\n",
    "                smoker_focal += 1\n",
    "\n",
    "    n_new = len(new_edges)\n",
    "    closure_rows.append({\n",
    "        'transition': f'{t_prev} -> {t_curr}',\n",
    "        'new_edges': n_new,\n",
    "        'triadic_closure_cases': triadic_count,\n",
    "        'membership_closure_cases': membership_count,\n",
    "        'focal_closure_cases': focal_count,\n",
    "        'membership_col_used': membership_col if membership_col else '',\n",
    "        'interest_cols_used': ','.join(interest_cols),\n",
    "        'smoker_involved_edges': smoker_involved_count,\n",
    "        'smoker_involved_triadic': smoker_triadic,\n",
    "        'smoker_involved_focal': smoker_focal,\n",
    "    })\n",
    "\n",
    "df_closure = pd.DataFrame(closure_rows)\n",
    "save_df(df_closure,OUTPUT_DIR_Q2 / 'closure_mechanisms.csv')\n"
   ],
   "id": "c88a14c05168ba70",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Part 3 — New Smoker Analysis",
   "id": "3d628a630a852314"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_smokes_col(df: pd.DataFrame) -> str:\n",
    "    for c in ['smokes', 'smoker', 'smoking']:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    raise ValueError('Smoking column not found.')\n",
    "\n",
    "def summarize_new_smokers(t_prev: str, t_curr: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    df_prev = data[t_prev]['df']\n",
    "    df_curr = data[t_curr]['df']\n",
    "    G_prev = data[t_prev]['G']\n",
    "\n",
    "    sm_col_prev = get_smokes_col(df_prev)\n",
    "    sm_col_curr = get_smokes_col(df_curr)\n",
    "\n",
    "    common_ids = df_prev.index.intersection(df_curr.index)\n",
    "    prev_sm = safe_binary_series(df_prev.loc[common_ids, sm_col_prev])\n",
    "    curr_sm = safe_binary_series(df_curr.loc[common_ids, sm_col_curr])\n",
    "\n",
    "    new_smoker_ids = common_ids[(prev_sm == 0) & (curr_sm == 1)].tolist()\n",
    "    stayed_nonsmoker_ids = common_ids[(prev_sm == 0) & (curr_sm == 0)].tolist()\n",
    "\n",
    "    candidate_cols = [c for c in ['studies','study','study_level','club','gender','plays_football','watches_movies','class_number','class'] if c in df_prev.columns]\n",
    "\n",
    "    rows = []\n",
    "    for uid in new_smoker_ids:\n",
    "        friends = list(G_prev.neighbors(uid)) if uid in G_prev else []\n",
    "        num_friends = len(friends)\n",
    "\n",
    "        smoker_friends = 0\n",
    "        for f in friends:\n",
    "            if f in df_prev.index:\n",
    "                smoker_friends += int(df_prev.loc[f, sm_col_prev]) == 1\n",
    "        pct_smoker_friends = (smoker_friends / num_friends * 100.0) if num_friends > 0 else np.nan\n",
    "\n",
    "        rec = {\n",
    "            'transition': f'{t_prev} -> {t_curr}',\n",
    "            'student_id': uid,\n",
    "            'num_friends_prev': num_friends,\n",
    "            'num_smoker_friends_prev': int(smoker_friends),\n",
    "            'pct_smoker_friends_prev': float(pct_smoker_friends) if not np.isnan(pct_smoker_friends) else np.nan,\n",
    "        }\n",
    "        for c in candidate_cols:\n",
    "            rec[c] = df_prev.loc[uid, c] if uid in df_prev.index else np.nan\n",
    "        rows.append(rec)\n",
    "\n",
    "    df_new = pd.DataFrame(rows)\n",
    "\n",
    "    base_rows = []\n",
    "    for uid in stayed_nonsmoker_ids:\n",
    "        friends = list(G_prev.neighbors(uid)) if uid in G_prev else []\n",
    "        num_friends = len(friends)\n",
    "        smoker_friends = 0\n",
    "        for f in friends:\n",
    "            if f in df_prev.index:\n",
    "                smoker_friends += int(df_prev.loc[f, sm_col_prev]) == 1\n",
    "        pct_smoker_friends = (smoker_friends / num_friends * 100.0) if num_friends > 0 else np.nan\n",
    "        base_rows.append({\n",
    "            'transition': f'{t_prev} -> {t_curr}',\n",
    "            'student_id': uid,\n",
    "            'num_friends_prev': num_friends,\n",
    "            'num_smoker_friends_prev': int(smoker_friends),\n",
    "            'pct_smoker_friends_prev': float(pct_smoker_friends) if not np.isnan(pct_smoker_friends) else np.nan,\n",
    "        })\n",
    "\n",
    "    df_base = pd.DataFrame(base_rows)\n",
    "    return df_new, df_base\n",
    "\n",
    "all_new = []\n",
    "all_base = []\n",
    "for i in range(len(day_tokens) - 1):\n",
    "    t_prev, t_curr = day_tokens[i], day_tokens[i+1]\n",
    "    df_new, df_base = summarize_new_smokers(t_prev, t_curr)\n",
    "    all_new.append(df_new)\n",
    "    all_base.append(df_base)\n",
    "\n",
    "df_new_smokers = pd.concat(all_new, ignore_index=True) if all_new else pd.DataFrame()\n",
    "df_baseline = pd.concat(all_base, ignore_index=True) if all_base else pd.DataFrame()\n",
    "\n",
    "save_df(df_new_smokers,OUTPUT_DIR_Q2 / 'new_smokers_analysis.csv')\n",
    "save_df(df_baseline,OUTPUT_DIR_Q2 / 'baseline_nonsmokers_sample.csv')\n",
    "\n",
    "print('New smokers rows:', len(df_new_smokers))"
   ],
   "id": "b89463588a9a5a50",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Part 4 — Centrality (Final Snapshot)",
   "id": "6c6cb074ae51057b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "final_tok = day_tokens[-1]\n",
    "G_final = data[final_tok]['G']\n",
    "df_final = data[final_tok]['df']\n",
    "\n",
    "deg_cent = nx.degree_centrality(G_final)\n",
    "top5 = sorted(deg_cent.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "# Pick a few attributes to export (if exist)\n",
    "attr_cols = [c for c in ['smokes','studies','study','study_level','club','gender','class_number','class'] if c in df_final.columns]\n",
    "\n",
    "rows = []\n",
    "for uid, score in top5:\n",
    "    rec = {'student_id': uid, 'degree_centrality': float(score), 'degree': int(G_final.degree(uid))}\n",
    "    if uid in df_final.index:\n",
    "        for c in attr_cols:\n",
    "            rec[c] = df_final.loc[uid, c]\n",
    "    rows.append(rec)\n",
    "\n",
    "df_top5 = pd.DataFrame(rows)\n",
    "save_df(df_top5,OUTPUT_DIR_Q2 / f'top_5_centrality_{final_tok}.csv')"
   ],
   "id": "ba0c657570361ca",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
